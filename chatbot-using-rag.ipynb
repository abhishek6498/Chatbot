{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abhishektiwari64/chatbot-using-rag?scriptVersionId=144784051\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem Statement: You are given a knowledge document over which you need to build a chatbot using Retrieval Augmented Generation (RAG). The RAG module should be capable of receiving user question as input and will need to output an answer to it based on the knowledge provided in the document","metadata":{}},{"cell_type":"markdown","source":"Let's install required libraries:\n\n1. LangChainâ€™s flexible abstractions and extensive toolkit enables developers to harness the power of LLMs.\n2. Pinecone is a cloud-based vector database service that provides a managed and scalable platform for similarity search and vector indexing\n3. openai provides with LLM\n4. The \"tiktoken\" library is a tool developed by OpenAI that allows you to count the number of tokens in a text string without making an API call to OpenAI's servers","metadata":{}},{"cell_type":"code","source":"pip install langchain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pinecone-client","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install openai","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tiktoken","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import langchain\nimport pinecone\n\n# TextLoader will be used to load data from Pan_card_services.txt to the document \nfrom langchain.document_loaders import TextLoader\n\n# CharacterTextSplitter will be used to chunkify the document\nfrom langchain.text_splitter import CharacterTextSplitter\n\n# OpenAIEmbeddings will be used to convert individual chunks and query to vectors\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n#Pinecone serves as vector-database\nfrom langchain.vectorstores import Pinecone\n\n#OpenAI will provide with LLM to generate response to the query\nfrom langchain import OpenAI\n\n#The RetrievalQAChain is a chain that combines a Retriever and a QA chain. \n# It is used to retrieve documents from a Retriever and then use a QA chain to answer a question \n# based on the retrieved documents.\nfrom langchain.chains import RetrievalQA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's load data from Pan_card_services.txt to the document ","metadata":{}},{"cell_type":"code","source":"loader = TextLoader(\"../input/knowledgedocuments/Pan_card_services.txt\")\ndocument = loader.load()\nprint(document)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"data is succefully loaded into the document\n\nLet's chunkify(split document into chunks) the document","metadata":{}},{"cell_type":"code","source":"text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=60)\ntexts = text_splitter.split_documents(document)\nprint(len(texts))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"document is split into 100 chunks","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nopenAi_api_key = user_secrets.get_secret(\"openAiKey\")\npinecone_api_key = user_secrets.get_secret(\"pineconeKey\")\n\n# set the env variable in order to use openAI module\nos.environ[\"OPENAI_API_KEY\"] = openAi_api_key\n\npinecone.init(api_key=pinecone_api_key, environment=\"gcp-starter\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will initialize OpenAIEmbeddings and create embeddings object to convert chunks into vectors","metadata":{}},{"cell_type":"code","source":"embeddings = OpenAIEmbeddings(openai_api_key=openAi_api_key)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"following will take the chunks, convert them into vectors using embeddings(Embedding model) object, and store the vectors into Pinecone","metadata":{}},{"cell_type":"code","source":"docsearch = Pinecone.from_documents(texts, embeddings, index_name=\"chatbot-rag-embeddings-index\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"following will:\n1. take query in prompt, convert query to vector and place this vector in vector database\n2. then top k vectors semantically similar to query vector will chosen\n3. top k vectors will be converted back to respective chunks and stuffed into prompt\n4. new prompt containing these k chunks and query will be fed to openAi LLM to generate response","metadata":{}},{"cell_type":"code","source":"qa = RetrievalQA.from_chain_type(llm = OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can ask questions and get answers","metadata":{}},{"cell_type":"code","source":"query = \"What is the cost/fees of a PAN card?\"\nresult = qa({\"query\":query})\nprint(result[\"result\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"Can I apply for a PAN card if I am a non-resident Indian (NRI)?\"\nresult = qa({\"query\":query})\nprint(result[\"result\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"How to apply for PAN card?\"\nresult = qa({\"query\":query})\nprint(result[\"result\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}